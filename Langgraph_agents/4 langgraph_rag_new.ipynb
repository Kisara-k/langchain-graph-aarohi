{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph RAG (Retrieval-Augmented Generation) Tutorial\n",
    "\n",
    "This notebook demonstrates building a RAG system using LangGraph with Hugging Face models. **RAG** combines the power of large language models with external knowledge retrieval to provide accurate, up-to-date responses.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Retrieval**: Finding relevant documents from a knowledge base\n",
    "- **Augmentation**: Enhancing LLM prompts with retrieved context\n",
    "- **Generation**: Producing responses based on both training data and retrieved information\n",
    "- **Vector Store**: Database optimized for semantic similarity search\n",
    "\n",
    "### What You'll Build:\n",
    "A complete RAG pipeline that loads web content, creates embeddings, and answers questions using retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup for RAG\n",
    "\n",
    "Install packages for building a complete RAG system:\n",
    "- **langchain-huggingface**: Interface to Hugging Face models and embeddings\n",
    "- **transformers & torch**: Core ML libraries for running models locally\n",
    "- **langgraph**: Orchestration framework for complex AI workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfVQ1bqlrsbE"
   },
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-core langchain_community langgraph langchain-huggingface transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loading\n",
    "\n",
    "Load content from web URLs using UnstructuredURLLoader. This extracts text from web pages and converts them into LangChain Document objects for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vmgx7z_5wiRF"
   },
   "outputs": [],
   "source": [
    "#!pip install unstructured\n",
    "\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "# Load LangGraph documentation for our knowledge base\n",
    "# UnstructuredURLLoader extracts and parses text content from web pages\n",
    "urls = ['https://langchain-ai.github.io/langgraph/tutorials/introduction/']\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JUUygcLSxcFU"
   },
   "outputs": [],
   "source": [
    "#docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chunking Strategy\n",
    "\n",
    "**Why split documents?** Large documents exceed model context limits and reduce retrieval precision. Chunking creates manageable pieces while preserving context:\n",
    "\n",
    "- **chunk_size=1000**: Optimal balance between context and efficiency\n",
    "- **chunk_overlap=200**: Prevents information loss at boundaries  \n",
    "- **RecursiveCharacterTextSplitter**: Intelligently splits at natural boundaries (paragraphs, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chunking\n",
    "\n",
    "Split documents into manageable chunks for embedding and retrieval. Proper chunking ensures each piece contains coherent information while maintaining context overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4QUZdTXyW1z",
    "outputId": "985319c1-5a61-4eb1-9b78-e16fc4ca4916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents:  98\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\"Total number of documents: \",len(all_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQA2Ve0MzeUZ",
    "outputId": "e01f868e-958c-43ab-909c-b0f93584a24f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/'}, page_content=\"Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It's particularly useful for developing more complex, stateful AI applications that go beyond simple query-response interactions.\\nGoodbye!\\n\\nCongratulations! You've built your first chatbot using LangGraph. This bot can engage in basic conversation by taking user input and generating responses using an LLM. You can inspect a LangSmith Trace for the call above at the provided link.\\n\\nHowever, you may have noticed that the bot's knowledge is limited to what's in its training data. In the next part, we'll add a web search tool to expand the bot's knowledge and make it more capable.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings: Converting Text to Vectors\n",
    "\n",
    "**Embeddings** transform text into numerical vectors that capture semantic meaning. Similar concepts cluster together in vector space, enabling semantic search:\n",
    "\n",
    "- **HuggingFaceEmbeddings**: Uses pre-trained models for text encoding\n",
    "- **Vector dimensions**: Each text chunk becomes a high-dimensional vector\n",
    "- **Semantic similarity**: Distance between vectors indicates conceptual relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KAjHFQaz5qb",
    "outputId": "93757038-dc17-4e38-9f2c-9aa136c344a6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-4c91a361ae8b>:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "<ipython-input-6-4c91a361ae8b>:5: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03492268547415733,\n",
       " 0.0188300758600235,\n",
       " -0.017854738980531693,\n",
       " 0.0001388332893839106,\n",
       " 0.07407363504171371]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Embedding models: https://python.langchain.com/v0.1/docs/integrations/text_embedding/\n",
    "\n",
    "# Let's load the Hugging Face Embedding class.  sentence_transformers\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "vector = embeddings.embed_query(\"hello, world!\")\n",
    "vector[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store: Building the Knowledge Base\n",
    "\n",
    "**ChromaDB** creates a searchable database of document embeddings:\n",
    "\n",
    "- **from_documents()**: Automatically embeds and indexes all document chunks\n",
    "- **Similarity search**: Finds most relevant chunks based on query vectors\n",
    "- **Persistence**: Optionally save the database locally to avoid recomputing embeddings\n",
    "\n",
    "The vector store is the foundation of retrieval - it makes semantic search possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8Je94sK0S2S",
    "outputId": "372ef40c-98e0-40c5-fcf6-1799fbfa305f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-43e6a2a5b884>:6: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  vectorstore = Chroma.from_documents(documents=all_splits, embedding=HuggingFaceEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "#!pip install langchain_chroma\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=HuggingFaceEmbeddings())\n",
    "\n",
    "# if you want to store chromadb locally\n",
    "\n",
    "# vectorstore = Chroma.from_documents(\n",
    "#     documents=docs,\n",
    "#     embedding=HuggingFaceEmbeddings(),\n",
    "#     persist_directory=\"./my_chroma_db\"  # Custom directory\n",
    "# )\n",
    "\n",
    "\n",
    "# Loading the Database Later\n",
    "# This reloads the previously stored embeddings so you don’t have to recompute them.\n",
    "\n",
    "# vectorstore = Chroma(\n",
    "#     persist_directory=\"./my_chroma_db\",\n",
    "#     embedding_function=HuggingFaceEmbeddings()\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local LLM Setup: Hugging Face Pipeline\n",
    "\n",
    "Using **local models** provides privacy and cost control compared to API-based solutions:\n",
    "\n",
    "- **falcon-7b**: Efficient open-source model suitable for RAG tasks\n",
    "- **torch_dtype=bfloat16**: Memory-optimized precision for faster inference\n",
    "- **max_new_tokens=200**: Limits response length for efficiency\n",
    "- **Temperature & top_k**: Controls response creativity and diversity\n",
    "\n",
    "Local models require GPU memory but eliminate API costs and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "fb26b24021d94ca4a9963d7f07d85bbe",
      "257482e8e5eb4d9db3824aacdd98e527",
      "a55671c5f18a49a0b952e4da114578d3",
      "aa833a277dc340578799712b9f98d366",
      "0d7107407929466c97d7acea3c0d051b",
      "2e2a0cae7b49453581fd45fdb7b43f87",
      "ac8da6f406c5446dbd335cd6b51750d4",
      "cf6ea32a2af04dbe9581b22f8f6a0fda",
      "489a4123a7654e4fa6f949c08abf835f",
      "5fc970bf41ca4fe0854d79f90e3e5e9f",
      "ac6a357ad5f84204b4c747a0e210cb72"
     ]
    },
    "id": "_qtSWzvD1Zc3",
    "outputId": "0d77bce2-c534-4a92-9bcf-d1b899ff84dc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb26b24021d94ca4a9963d7f07d85bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "#model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "model_id = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# text_generation_pipeline = pipeline(\n",
    "#     \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, max_new_tokens=400, device=0)\n",
    "\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    max_new_tokens=200,\n",
    "    device=0,\n",
    "    temperature=0.7,  #  (lower values = more deterministic)\n",
    "    top_k=50,  # Filters out low-probability tokens\n",
    ")\n",
    "\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Prompt Template\n",
    "\n",
    "**Prompt engineering** is crucial for RAG success. The prompt template structures how retrieved context is presented to the LLM:\n",
    "\n",
    "- **Context injection**: Places relevant documents before the question\n",
    "- **Instructions**: Guides the model on how to use the context\n",
    "- **Constraints**: Prevents hallucination by limiting responses to provided context\n",
    "- **Hub.pull()**: Uses community-tested prompts for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKq_3VbE2JgU",
    "outputId": "e7ed96d2-ab3d-44fa-e625-be3a9d54960a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "# If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "# Use three sentences maximum and keep the answer as concise as possible.\n",
    "# Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Helpful Answer:\"\"\"\n",
    "# prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph State Management\n",
    "\n",
    "**State** defines the data structure that flows through the RAG workflow. TypedDict provides type safety and clear contracts:\n",
    "\n",
    "- **question**: User's input query\n",
    "- **context**: Retrieved documents from vector store\n",
    "- **answer**: Final LLM response\n",
    "\n",
    "State management ensures data consistency and enables complex multi-step workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9CJgNteN2yN8"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Retrieval Function\n",
    "\n",
    "The **retrieval step** finds relevant documents based on semantic similarity:\n",
    "\n",
    "- **similarity_search()**: Queries vector store with the user's question\n",
    "- **k=1**: Retrieves top 1 most relevant document (adjust based on context window)\n",
    "- **State update**: Adds retrieved documents to the workflow state\n",
    "\n",
    "This step transforms user questions into relevant knowledge context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gBiZd2F93FDR"
   },
   "outputs": [],
   "source": [
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vectorstore.similarity_search(state[\"question\"],  k=1)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generation Function\n",
    "\n",
    "The **generation step** combines retrieved context with the user's question to produce an informed response:\n",
    "\n",
    "- **Document concatenation**: Joins retrieved content into coherent context\n",
    "- **Prompt invocation**: Structures the context and question using the RAG template\n",
    "- **LLM inference**: Generates response based on both training data and retrieved context\n",
    "\n",
    "This step completes the RAG pipeline by grounding the LLM's response in retrieved knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oCvkyhGV3jvV"
   },
   "outputs": [],
   "source": [
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    #return {\"answer\": response.content}\n",
    "    return {\"answer\": response}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Compilation: Building the RAG Graph\n",
    "\n",
    "**LangGraph orchestration** connects retrieval and generation into a seamless pipeline:\n",
    "\n",
    "- **StateGraph**: Defines the workflow structure with typed state\n",
    "- **add_sequence()**: Creates linear flow: retrieve → generate\n",
    "- **Compilation**: Optimizes the graph for execution\n",
    "- **START edge**: Defines the entry point (retrieve step)\n",
    "\n",
    "The graph ensures proper data flow and error handling between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oY9A3sHo4JLd"
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "zmUfbMvW4VE5",
    "outputId": "6ed501ed-d6d9-4b0f-efde-c6510342b8c1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Complete RAG System\n",
    "\n",
    "Execute the full RAG pipeline with a question about LangGraph. The system will:\n",
    "1. **Retrieve** relevant documentation from the vector store\n",
    "2. **Generate** a response using both retrieved context and the LLM's knowledge\n",
    "3. **Return** a grounded, accurate answer based on the loaded documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4IXeIYb64cW5",
    "outputId": "f4a97aad-1f36-41b9-8ccc-d2d5a33b0421"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: what is langgraph? \n",
      "Context: Thank you for your patience. I've found some recent information about LangGraph for you. Let me summarize the key points:\n",
      "\n",
      "1. LangGraph is part of the LangChain ecosystem, which is a leading platform in AI development.\n",
      "\n",
      "2. Recent updates and features of LangGraph include:\n",
      "\n",
      "   a. LangGraph Cloud: This seems to be a cloud-based version of LangGraph, though specific details weren't provided in the search results.\n",
      "\n",
      "   b. LangGraph Platform: This is a newly introduced concept that combines several offerings:\n",
      "      - LangGraph Server\n",
      "      - LangGraph Studio\n",
      "      - CLI (Command Line Interface)\n",
      "      - SDK (Software Development Kit)\n",
      "\n",
      "3. LangGraph Server: This component has received new features to enhance its value proposition, though the specific features weren't detailed in the search results.\n",
      "\n",
      "4. LangGraph Studio: This appears to be a new tool in the LangGraph ecosystem, likely providing a graphical interface for working with LangGraph. \n",
      "Answer: LangGraph is a cloud-based platform for AI development. It includes a server, a studio, and a CLI. LangGraph Studio is a new tool in the LangGraph ecosystem.\n",
      "\n",
      "Thank you for your patience. I've found some recent information about LangGraph for you. Let me summarize the key points:\n",
      "\n",
      "1. LangGraph is part of the LangChain ecosystem, which is a leading platform in AI development.\n",
      "\n",
      "2. Recent updates and features of LangGraph include:\n",
      "\n",
      "a. LangGraph Cloud: This seems to be a cloud-based version of LangGraph, though specific details weren't provided in the search results.\n",
      "\n",
      "b. LangGraph Platform: This is a newly introduced concept that combines several offerings:\n",
      "\n",
      "c. LangGraph Server: This component has received new features to enhance its value proposition, though the specific features weren't detailed in the search results.\n",
      "\n",
      "d. LangGraph Studio: This appears to be a new\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"what is langgraph?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d7107407929466c97d7acea3c0d051b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "257482e8e5eb4d9db3824aacdd98e527": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2e2a0cae7b49453581fd45fdb7b43f87",
      "placeholder": "​",
      "style": "IPY_MODEL_ac8da6f406c5446dbd335cd6b51750d4",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "2e2a0cae7b49453581fd45fdb7b43f87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "489a4123a7654e4fa6f949c08abf835f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5fc970bf41ca4fe0854d79f90e3e5e9f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a55671c5f18a49a0b952e4da114578d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf6ea32a2af04dbe9581b22f8f6a0fda",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_489a4123a7654e4fa6f949c08abf835f",
      "value": 2
     }
    },
    "aa833a277dc340578799712b9f98d366": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fc970bf41ca4fe0854d79f90e3e5e9f",
      "placeholder": "​",
      "style": "IPY_MODEL_ac6a357ad5f84204b4c747a0e210cb72",
      "value": " 2/2 [00:01&lt;00:00,  1.30it/s]"
     }
    },
    "ac6a357ad5f84204b4c747a0e210cb72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac8da6f406c5446dbd335cd6b51750d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf6ea32a2af04dbe9581b22f8f6a0fda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb26b24021d94ca4a9963d7f07d85bbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_257482e8e5eb4d9db3824aacdd98e527",
       "IPY_MODEL_a55671c5f18a49a0b952e4da114578d3",
       "IPY_MODEL_aa833a277dc340578799712b9f98d366"
      ],
      "layout": "IPY_MODEL_0d7107407929466c97d7acea3c0d051b"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
